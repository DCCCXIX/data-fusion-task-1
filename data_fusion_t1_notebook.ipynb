{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 55555\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet('data_fusion_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled dataset\n",
    "dataset = dataset[dataset.category_id != -1].drop_duplicates(\"item_name\").reset_index(drop = True)\n",
    "\n",
    "#encoding values into a range from 0 to 95\n",
    "labelecoder = LabelEncoder()\n",
    "dataset[\"category_id\"] = labelecoder.fit_transform(dataset[\"category_id\"])\n",
    "\n",
    "label_counts = dataset[\"category_id\"].value_counts()\n",
    "label_counts.plot.hist(bins = len(label_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preproccesses the dataframe, generates features\n",
    "def preproccess(dataset):\n",
    "     replace_dict = {\n",
    "          \"w\" : \"ш\",\n",
    "          \"e\" : \"е\",\n",
    "          \"y\" : \"у\",\n",
    "          \"u\" : \"и\",\n",
    "          \"o\" : \"о\",\n",
    "          \"p\" : \"р\",\n",
    "          \"a\" : \"а\",\n",
    "          \"h\" : \"н\",\n",
    "          \"k\" : \"к\",\n",
    "          \"x\" : \"х\",\n",
    "          \"c\" : \"с\",\n",
    "          \"b\" : \"в\",\n",
    "          \"m\" : \"м\",\n",
    "          \"r\" : \"г\",\n",
    "          \"a\" : \"a\",\n",
    "          \"a\" : \"a\",          \n",
    "     }\n",
    "     #replaces latin letters in cyrillic words\n",
    "     def replace_latin(text):\n",
    "          if len(text) > 0:\n",
    "               text = text.split()\n",
    "               words = []\n",
    "               for word in text:\n",
    "                    if re.match(r\"^[a-z]+$\", word):\n",
    "                         pass\n",
    "                    else:\n",
    "                         for lat, ru in replace_dict.items():\n",
    "                              word = word.replace(lat, ru)\n",
    "                    words.append(word)\n",
    "               \n",
    "               return \" \".join(words)\n",
    "          else:\n",
    "               return \"none\"\n",
    "     #separates numbers and punctuation from words\n",
    "     def pretokenize_test(text):          \n",
    "          text = text.replace(\"-\", \"\")\n",
    "          text = re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", text).strip()\n",
    "          text = \"\".join(c if c not in string.punctuation else f\" {c} \" for c in text )\n",
    "          text = \"\".join(c if c not in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] else f\" {c} \" for c in text)\n",
    "          #text = re.sub(r'[0-9]', ' 1 ', text)\n",
    "          text =  \" \".join(w.strip() for w in text.split())         \n",
    "\n",
    "          return text\n",
    "     #removes all numbers and punctuation\n",
    "     def clean_text(text):\n",
    "          text = re.sub(r'[?|!\"|#|.|%|,|$|~|^|`|;|@|╣|)|(|\\\\|\\||-|[|\\]|{|}|:|<|>|\\'|+|&|=|°|/|№|\\-|*|_]', r\" \", text)\n",
    "          text = re.sub(r'[0-9]', '', text)          \n",
    "          return text\n",
    "     #removes all words shorter than 2 letters\n",
    "     def keep_long_words(text):\n",
    "          words = []\n",
    "          for word in text.split():\n",
    "               if len(word) > 2:\n",
    "                    words.append(word)\n",
    "          if len(words) == 0:\n",
    "               return \"none\"\n",
    "          return \" \".join(words)\n",
    "\n",
    "     #split time into hours and minutes\n",
    "     time_list = [x.split(\":\") for x in dataset['receipt_time'].values]\n",
    "     dataset['receipt_time_hours'] = [row[0] for row in time_list]\n",
    "     dataset['receipt_time_minutes'] = [row[1] for row in time_list]\n",
    "     #drop unneccessary \"receipt time\" column\n",
    "     dataset.drop([\"receipt_time\"], inplace = True, axis = 1)\n",
    "\n",
    "     #creating features\n",
    "     #order of execution is important\n",
    "     dataset[\"string_length\"] = dataset[\"item_name\"].apply(lambda x: len(x))\n",
    "     dataset[\"punctuation_count\"] = dataset[\"item_name\"].apply(lambda comment: sum(comment.count(w) for w in '.,;:-/\\\\'))\n",
    "     dataset[\"uppercase_amount\"] = dataset[\"item_name\"].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.lower())\n",
    "     dataset[\"digit_amount\"] = dataset[\"item_name\"].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: pretokenize_test(x))\n",
    "\n",
    "     dataset[\"latin_amount\"] = dataset[\"item_name\"].apply(lambda x: len(re.findall(r\"[a-z]\", x)))\n",
    "     dataset[\"cyrillic_amount\"] = dataset[\"item_name\"].apply(lambda x: len(re.findall(r\"[а-я]\", x)))          \n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"ё\", \"е\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"й\", \"и\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"ъ\", \"ь\"))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"tpk\", \"трк\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"трк\", \"топливо\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\" аи \", \" бензин \"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"раф\", \"кофе\"))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: replace_latin(x))\n",
    "     \n",
    "     dataset[\"temp_col\"] = dataset[\"item_name\"].apply(lambda x: clean_text(x))     \n",
    "     \n",
    "     dataset[\"has_kcal\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"ккал\" in x.split() else 0)\n",
    "     dataset[\"has_kg\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"кг\" in x.split() else 0)\n",
    "     dataset[\"has_g\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"г\" in x.split() else 0)\n",
    "     dataset[\"has_ml\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"мл\" in x.split() else 0)    \n",
    "     dataset[\"has_l\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"л\" in x.split() else 0)\n",
    "     dataset[\"has_sht\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"шт\" in x.split() else 0)\n",
    "     dataset[\"has_tab\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"таб\" in x.split() else 0)\n",
    "     dataset[\"has_sb\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"сб\" in x.split() else 0)\n",
    "\n",
    "     dataset[\"has_fl\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"фл\" in x.split() else 0)\n",
    "     dataset[\"has_up\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"уп\" in x.split() else 0)\n",
    "     dataset[\"has_cm\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"см\" in x.split() else 0)\n",
    "     dataset[\"has_m\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"м\" in x.split() else 0)\n",
    "     dataset[\"has_mm\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"мм\" in x.split() else 0)\n",
    "\n",
    "     dataset[\"temp_col\"] = dataset[\"temp_col\"].apply(lambda x: keep_long_words(x))\n",
    "\n",
    "     dataset[\"first_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[0] if len(x.split()) > 0 else \"none\")\n",
    "     dataset[\"last_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[-1] if len(x.split()) > 0 else \"none\")\n",
    "\n",
    "     dataset[\"second_first_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[1] if len(x.split()) > 1 else \"none\")\n",
    "     dataset[\"second_last_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[-2] if len(x.split()) > 1 else \"none\")\n",
    "\n",
    "     dataset.drop(\"temp_col\", axis = 1, inplace = True)\n",
    "\n",
    "     return dataset\n",
    "\n",
    "dataset = preproccess(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preproccessing might create duplicates\n",
    "dataset.drop_duplicates(\"item_name\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower dtypes to save memory\n",
    "def lower_dtypes(dataset):\n",
    "\n",
    "    dtype_dict = {\n",
    "    \"has_kcal\" : np.int8,\n",
    "    \"has_kg\" : np.int8,\n",
    "    \"has_g\" : np.int8,\n",
    "    \"has_ml\" : np.int8,\n",
    "    \"has_l\" : np.int8,\n",
    "    \"has_sht\" : np.int8,\n",
    "    \"has_tab\" : np.int8,\n",
    "    \"has_sb\" : np.int8,\n",
    "    \"has_fl\" : np.int8,\n",
    "    \"has_up\" : np.int8,\n",
    "    \"has_cm\" : np.int8,\n",
    "    \"has_m\" : np.int8,\n",
    "    \"has_mm\" : np.int8,\n",
    "    \"receipt_id\" : np.int32,\n",
    "    \"id\" : np.int32,\n",
    "    \"receipt_dayofweek\" : np.int8,\n",
    "    \"one_char_amount\" : np.int8,\n",
    "    \"two_char_amount\" : np.int8,\n",
    "    \"item_name\" : str,\n",
    "    \"item_name_modified\" : str,\n",
    "    \"item_quantity\" : np.int8,\n",
    "    \"item_price\" : np.int8,\n",
    "    \"item_nds_rate\" : np.int8,\n",
    "    \"category_id\" : np.int8,\n",
    "    \"receipt_time_hours\" : np.int8,\n",
    "    \"receipt_time_minutes\" : np.int8,\n",
    "    \"string_length\" : np.float16,\n",
    "    \"punctuation_count\" : np.int8,\n",
    "    \"uppercase_amount\" : np.int8,\n",
    "    \"FULLCAPS_COUNT\" : np.float16,\n",
    "    \"digit_amount\" : np.int8,\n",
    "    \"relative_digit_amount\" : np.float16,\n",
    "    \"latin_amount\" : np.int8,\n",
    "    \"cyrillic_amount\" : np.int8,\n",
    "    \"relative_latin_amount\" : np.float16,\n",
    "    \"relative_cyrillic_amount\" : np.float16,\n",
    "    \"first_word\" : str,\n",
    "    \"last_word\" : str,\n",
    "    \"second_first_word\" : str,\n",
    "    \"second_last_word\" : str,\n",
    "    \"item_name_list\" : str,\n",
    "    \"two_letter_features\" : str,\n",
    "    \"one_letter_features\" : str,\n",
    "    \"brands\" : str\n",
    "    }\n",
    "   \n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].astype(dtype_dict[column])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = lower_dtypes(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = train_test_split(dataset, test_size = 0.1, shuffle = True, stratify = dataset[\"category_id\"], random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.026163543"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "#train dataset memory consumption\n",
    "sum(train_dataset.memory_usage(deep=True).values)/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting features into categorical, continuous and text\n",
    "cont_features = [\n",
    "                \"string_length\",\n",
    "                \"punctuation_count\",\n",
    "                \"uppercase_amount\",\n",
    "                \"digit_amount\",\n",
    "                \"latin_amount\",\n",
    "                \"cyrillic_amount\",\n",
    "                \"item_quantity\",\n",
    "                \"item_price\"\n",
    "                ]\n",
    "\n",
    "cat_features = [\n",
    "                \"has_kcal\",\n",
    "                \"has_kg\",\n",
    "                \"has_g\",\n",
    "                \"has_ml\",\n",
    "                \"has_l\",\n",
    "                \"has_sht\",\n",
    "                \"has_tab\",\n",
    "                \"has_sb\",\n",
    "                \"has_fl\",\n",
    "                \"has_up\",\n",
    "                \"has_cm\",\n",
    "                \"has_m\",\n",
    "                \"has_mm\",\n",
    "                \"item_nds_rate\",                             \n",
    "                \"receipt_time_hours\",\n",
    "                \"receipt_time_minutes\",                \n",
    "                \"receipt_dayofweek\",\n",
    "                #\"receipt_id\"\n",
    "                ]\n",
    "\n",
    "text_features = [\"item_name\", \"first_word\", \"last_word\", \"second_first_word\", \"second_last_word\"]\n",
    "target = [\"category_id\"]\n",
    "\n",
    "features = text_features + cat_features + cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom text proccessing\n",
    "#text processing differs for item_name and other text columns\n",
    "text_processing = {\n",
    "        \"tokenizers\" : [\n",
    "          {\n",
    "            \"tokenizer_id\" : \"Space\",\n",
    "            \"separator_type\" : \"ByDelimiter\",\n",
    "            \"delimiter\" : \" \",\n",
    "            \"number_process_policy\" : \"Replace\",\n",
    "            \"number_token\" : \"@\",\n",
    "          },          \n",
    "        ],\n",
    "\n",
    "        \"dictionaries\" : [\n",
    "            {            \n",
    "              \"dictionary_id\" : \"Unigram\",\n",
    "              \"token_level_type\": \"Letter\",\n",
    "              #\"max_dictionary_size\" : \"500\",\n",
    "              \"occurrence_lower_bound\" : \"1\",\n",
    "              \"gram_order\" : \"1\"\n",
    "            },          \n",
    "            {\n",
    "            \"dictionary_id\" : \"Bigram\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"2\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Trigram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"3\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Fourgram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"4\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Fivegram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"5\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Sixgram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"6\"\n",
    "            },   \n",
    "        ],\n",
    "\n",
    "        \"feature_processing\" : {\n",
    "            \"item_name\" : [                  \n",
    "                  {\n",
    "                    \"dictionaries_names\" : [\n",
    "                      #\"Unigram\",                      \n",
    "                      #\"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BoW\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [ \n",
    "                      #\"Unigram\",                     \n",
    "                      #\"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"NaiveBayes\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [                      \n",
    "                      #\"Unigram\",\n",
    "                      #\"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BM25\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "            ],\n",
    "\n",
    "            \"default\" : [                  \n",
    "                  {\n",
    "                    \"dictionaries_names\" : [\n",
    "                      #\"Unigram\",                      \n",
    "                      \"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BoW\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [ \n",
    "                      #\"Unigram\",                     \n",
    "                      \"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"NaiveBayes\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [                      \n",
    "                      #\"Unigram\",\n",
    "                      \"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BM25\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "            ],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "params = {\n",
    "    \"iterations\" : 10000,\n",
    "    \"text_processing\" : text_processing,\n",
    "    #\"auto_class_weights\" : \"Balanced\",\n",
    "    # \"boosting_type\" : \"Plain\",\n",
    "    \"max_ctr_complexity\" : 2,\n",
    "    \"bootstrap_type\" : \"Bernoulli\",\n",
    "    \"subsample\" : 0.1,\n",
    "    \"gpu_cat_features_storage\" : \"CpuPinnedMemory\",\n",
    "    \"depth\" : 4, #7\n",
    "    \"task_type\" : \"GPU\",\n",
    "    \"nan_mode\" : \"Min\",\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"l2_leaf_reg\" : 80,\n",
    "    \"loss_function\" : \"MultiClass\",\n",
    "    \"eval_metric\" : \"TotalF1\",\n",
    "    \"random_seed\" : seed,\n",
    "    \"verbose\" : True,\n",
    "    \"metric_period\" : 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\tlearn: 0.1440711\ttest: 0.1461459\tbest: 0.1461459 (0)\ttotal: 1.36s\tremaining: 3h 47m 12s\n",
      "50:\tlearn: 0.5378339\ttest: 0.5643070\tbest: 0.5643070 (50)\ttotal: 1m 1s\tremaining: 3h 20m 26s\n",
      "100:\tlearn: 0.6002669\ttest: 0.6283498\tbest: 0.6283498 (100)\ttotal: 1m 59s\tremaining: 3h 14m 50s\n",
      "150:\tlearn: 0.6494578\ttest: 0.6822945\tbest: 0.6822945 (150)\ttotal: 2m 58s\tremaining: 3h 14m 7s\n",
      "200:\tlearn: 0.6773439\ttest: 0.7118043\tbest: 0.7118043 (200)\ttotal: 4m 1s\tremaining: 3h 16m 25s\n",
      "250:\tlearn: 0.6924897\ttest: 0.7309034\tbest: 0.7309034 (250)\ttotal: 5m 8s\tremaining: 3h 19m 33s\n",
      "300:\tlearn: 0.7051838\ttest: 0.7447730\tbest: 0.7447730 (300)\ttotal: 6m 14s\tremaining: 3h 21m 14s\n",
      "350:\tlearn: 0.7212813\ttest: 0.7624073\tbest: 0.7624073 (350)\ttotal: 7m 20s\tremaining: 3h 21m 51s\n",
      "400:\tlearn: 0.7288138\ttest: 0.7726487\tbest: 0.7726487 (400)\ttotal: 8m 30s\tremaining: 3h 23m 39s\n",
      "450:\tlearn: 0.7329052\ttest: 0.7758348\tbest: 0.7758348 (450)\ttotal: 9m 36s\tremaining: 3h 23m 36s\n",
      "500:\tlearn: 0.7377335\ttest: 0.7800576\tbest: 0.7800576 (500)\ttotal: 10m 45s\tremaining: 3h 23m 54s\n",
      "550:\tlearn: 0.7409668\ttest: 0.7837085\tbest: 0.7837085 (550)\ttotal: 11m 52s\tremaining: 3h 23m 31s\n",
      "600:\tlearn: 0.7430762\ttest: 0.7875647\tbest: 0.7875647 (600)\ttotal: 13m\tremaining: 3h 23m 21s\n",
      "650:\tlearn: 0.7456768\ttest: 0.7877044\tbest: 0.7877044 (650)\ttotal: 14m 11s\tremaining: 3h 23m 41s\n",
      "700:\tlearn: 0.7487034\ttest: 0.7916206\tbest: 0.7916206 (700)\ttotal: 15m 21s\tremaining: 3h 23m 47s\n",
      "750:\tlearn: 0.7508264\ttest: 0.7946076\tbest: 0.7946076 (750)\ttotal: 16m 31s\tremaining: 3h 23m 31s\n",
      "800:\tlearn: 0.7528385\ttest: 0.7958073\tbest: 0.7958073 (800)\ttotal: 17m 40s\tremaining: 3h 23m 4s\n",
      "850:\tlearn: 0.7539137\ttest: 0.7972606\tbest: 0.7972606 (850)\ttotal: 18m 51s\tremaining: 3h 22m 42s\n",
      "900:\tlearn: 0.7555599\ttest: 0.7990903\tbest: 0.7990903 (900)\ttotal: 19m 59s\tremaining: 3h 21m 56s\n",
      "950:\tlearn: 0.7571111\ttest: 0.8014792\tbest: 0.8014792 (950)\ttotal: 21m 8s\tremaining: 3h 21m 14s\n",
      "1000:\tlearn: 0.7587024\ttest: 0.8019206\tbest: 0.8019206 (1000)\ttotal: 22m 16s\tremaining: 3h 20m 14s\n",
      "1050:\tlearn: 0.7606817\ttest: 0.8016772\tbest: 0.8019206 (1000)\ttotal: 23m 25s\tremaining: 3h 19m 25s\n",
      "1100:\tlearn: 0.7616834\ttest: 0.8022459\tbest: 0.8022459 (1100)\ttotal: 24m 33s\tremaining: 3h 18m 31s\n",
      "1150:\tlearn: 0.7629256\ttest: 0.8034393\tbest: 0.8034393 (1150)\ttotal: 25m 33s\tremaining: 3h 16m 28s\n",
      "1200:\tlearn: 0.7646760\ttest: 0.8044583\tbest: 0.8044583 (1200)\ttotal: 26m 30s\tremaining: 3h 14m 8s\n",
      "1250:\tlearn: 0.7658583\ttest: 0.8066020\tbest: 0.8066020 (1250)\ttotal: 27m 26s\tremaining: 3h 11m 54s\n",
      "1300:\tlearn: 0.7668108\ttest: 0.8067018\tbest: 0.8067018 (1300)\ttotal: 28m 24s\tremaining: 3h 9m 58s\n",
      "1350:\tlearn: 0.7680447\ttest: 0.8070950\tbest: 0.8070950 (1350)\ttotal: 29m 21s\tremaining: 3h 7m 55s\n",
      "1400:\tlearn: 0.7689596\ttest: 0.8075086\tbest: 0.8075086 (1400)\ttotal: 30m 18s\tremaining: 3h 6m\n",
      "1450:\tlearn: 0.7704908\ttest: 0.8084050\tbest: 0.8084050 (1450)\ttotal: 31m 15s\tremaining: 3h 4m 11s\n",
      "1500:\tlearn: 0.7710321\ttest: 0.8098023\tbest: 0.8098023 (1500)\ttotal: 32m 11s\tremaining: 3h 2m 17s\n",
      "1550:\tlearn: 0.7715907\ttest: 0.8099161\tbest: 0.8099161 (1550)\ttotal: 33m 9s\tremaining: 3h 38s\n",
      "1600:\tlearn: 0.7729190\ttest: 0.8103196\tbest: 0.8103196 (1600)\ttotal: 34m 8s\tremaining: 2h 59m 6s\n",
      "1650:\tlearn: 0.7745553\ttest: 0.8115996\tbest: 0.8115996 (1650)\ttotal: 35m 5s\tremaining: 2h 57m 29s\n",
      "1700:\tlearn: 0.7757811\ttest: 0.8115209\tbest: 0.8115996 (1650)\ttotal: 36m 3s\tremaining: 2h 55m 55s\n",
      "1750:\tlearn: 0.7763114\ttest: 0.8125709\tbest: 0.8125709 (1750)\ttotal: 37m 3s\tremaining: 2h 54m 34s\n",
      "1800:\tlearn: 0.7771256\ttest: 0.8133090\tbest: 0.8133090 (1800)\ttotal: 38m 1s\tremaining: 2h 53m 8s\n",
      "1850:\tlearn: 0.7776849\ttest: 0.8131978\tbest: 0.8133090 (1800)\ttotal: 39m 2s\tremaining: 2h 51m 52s\n",
      "1900:\tlearn: 0.7783148\ttest: 0.8142054\tbest: 0.8142054 (1900)\ttotal: 40m\tremaining: 2h 50m 28s\n",
      "1950:\tlearn: 0.7789874\ttest: 0.8144299\tbest: 0.8144299 (1950)\ttotal: 40m 57s\tremaining: 2h 48m 58s\n",
      "2000:\tlearn: 0.7804189\ttest: 0.8156080\tbest: 0.8156080 (2000)\ttotal: 41m 58s\tremaining: 2h 47m 47s\n",
      "2050:\tlearn: 0.7816994\ttest: 0.8167205\tbest: 0.8167205 (2050)\ttotal: 42m 57s\tremaining: 2h 46m 29s\n",
      "2100:\tlearn: 0.7824361\ttest: 0.8173275\tbest: 0.8173275 (2100)\ttotal: 43m 56s\tremaining: 2h 45m 10s\n",
      "2150:\tlearn: 0.7830948\ttest: 0.8177153\tbest: 0.8177153 (2150)\ttotal: 44m 54s\tremaining: 2h 43m 53s\n",
      "2200:\tlearn: 0.7839945\ttest: 0.8180156\tbest: 0.8180156 (2200)\ttotal: 45m 53s\tremaining: 2h 42m 38s\n",
      "2250:\tlearn: 0.7845832\ttest: 0.8188921\tbest: 0.8188921 (2250)\ttotal: 46m 52s\tremaining: 2h 41m 21s\n",
      "2300:\tlearn: 0.7855355\ttest: 0.8191941\tbest: 0.8191941 (2300)\ttotal: 47m 49s\tremaining: 2h 40m 2s\n",
      "2350:\tlearn: 0.7862710\ttest: 0.8195402\tbest: 0.8195402 (2350)\ttotal: 48m 50s\tremaining: 2h 38m 52s\n",
      "2400:\tlearn: 0.7868323\ttest: 0.8198915\tbest: 0.8198915 (2400)\ttotal: 49m 48s\tremaining: 2h 37m 37s\n",
      "2450:\tlearn: 0.7875933\ttest: 0.8213255\tbest: 0.8213255 (2450)\ttotal: 50m 48s\tremaining: 2h 36m 30s\n",
      "2500:\tlearn: 0.7884228\ttest: 0.8224672\tbest: 0.8224672 (2500)\ttotal: 51m 57s\tremaining: 2h 35m 47s\n",
      "2550:\tlearn: 0.7893792\ttest: 0.8224386\tbest: 0.8224672 (2500)\ttotal: 53m\tremaining: 2h 34m 47s\n",
      "2600:\tlearn: 0.7902476\ttest: 0.8235196\tbest: 0.8235196 (2600)\ttotal: 54m 13s\tremaining: 2h 34m 14s\n",
      "2650:\tlearn: 0.7909159\ttest: 0.8235255\tbest: 0.8235255 (2650)\ttotal: 55m 20s\tremaining: 2h 33m 23s\n",
      "2700:\tlearn: 0.7916810\ttest: 0.8234543\tbest: 0.8235255 (2650)\ttotal: 56m 27s\tremaining: 2h 32m 34s\n",
      "2750:\tlearn: 0.7923886\ttest: 0.8235413\tbest: 0.8235413 (2750)\ttotal: 57m 38s\tremaining: 2h 31m 52s\n",
      "2800:\tlearn: 0.7931794\ttest: 0.8247337\tbest: 0.8247337 (2800)\ttotal: 58m 42s\tremaining: 2h 30m 53s\n",
      "2850:\tlearn: 0.7938693\ttest: 0.8251836\tbest: 0.8251836 (2850)\ttotal: 59m 49s\tremaining: 2h 30m\n",
      "2900:\tlearn: 0.7948174\ttest: 0.8252234\tbest: 0.8252234 (2900)\ttotal: 1h 54s\tremaining: 2h 29m 2s\n",
      "2950:\tlearn: 0.7952741\ttest: 0.8261621\tbest: 0.8261621 (2950)\ttotal: 1h 1m 58s\tremaining: 2h 28m 2s\n",
      "3000:\tlearn: 0.7960532\ttest: 0.8267875\tbest: 0.8267875 (3000)\ttotal: 1h 3m 9s\tremaining: 2h 27m 17s\n",
      "3050:\tlearn: 0.7969671\ttest: 0.8269017\tbest: 0.8269017 (3050)\ttotal: 1h 4m 18s\tremaining: 2h 26m 28s\n",
      "3100:\tlearn: 0.7975653\ttest: 0.8272979\tbest: 0.8272979 (3100)\ttotal: 1h 5m 22s\tremaining: 2h 25m 27s\n",
      "3150:\tlearn: 0.7983099\ttest: 0.8276938\tbest: 0.8276938 (3150)\ttotal: 1h 6m 24s\tremaining: 2h 24m 19s\n",
      "3200:\tlearn: 0.7989866\ttest: 0.8275974\tbest: 0.8276938 (3150)\ttotal: 1h 7m 36s\tremaining: 2h 23m 36s\n",
      "3250:\tlearn: 0.7996192\ttest: 0.8281084\tbest: 0.8281084 (3250)\ttotal: 1h 8m 37s\tremaining: 2h 22m 28s\n",
      "3300:\tlearn: 0.8002494\ttest: 0.8284587\tbest: 0.8284587 (3300)\ttotal: 1h 9m 45s\tremaining: 2h 21m 33s\n",
      "3350:\tlearn: 0.8014924\ttest: 0.8286976\tbest: 0.8286976 (3350)\ttotal: 1h 10m 52s\tremaining: 2h 20m 36s\n",
      "3400:\tlearn: 0.8020369\ttest: 0.8287960\tbest: 0.8287960 (3400)\ttotal: 1h 12m 1s\tremaining: 2h 19m 44s\n",
      "3450:\tlearn: 0.8029310\ttest: 0.8292665\tbest: 0.8292665 (3450)\ttotal: 1h 13m 12s\tremaining: 2h 18m 55s\n",
      "3500:\tlearn: 0.8034721\ttest: 0.8293214\tbest: 0.8293214 (3500)\ttotal: 1h 14m 22s\tremaining: 2h 18m 3s\n",
      "3550:\tlearn: 0.8043386\ttest: 0.8302668\tbest: 0.8302668 (3550)\ttotal: 1h 15m 33s\tremaining: 2h 17m 13s\n",
      "3600:\tlearn: 0.8046519\ttest: 0.8301233\tbest: 0.8302668 (3550)\ttotal: 1h 16m 43s\tremaining: 2h 16m 19s\n",
      "3650:\tlearn: 0.8053431\ttest: 0.8298654\tbest: 0.8302668 (3550)\ttotal: 1h 17m 55s\tremaining: 2h 15m 30s\n",
      "3700:\tlearn: 0.8060100\ttest: 0.8301481\tbest: 0.8302668 (3550)\ttotal: 1h 19m 7s\tremaining: 2h 14m 39s\n",
      "3750:\tlearn: 0.8068289\ttest: 0.8306285\tbest: 0.8306285 (3750)\ttotal: 1h 20m 16s\tremaining: 2h 13m 43s\n",
      "3800:\tlearn: 0.8073243\ttest: 0.8307205\tbest: 0.8307205 (3800)\ttotal: 1h 21m 25s\tremaining: 2h 12m 47s\n",
      "3850:\tlearn: 0.8081416\ttest: 0.8299413\tbest: 0.8307205 (3800)\ttotal: 1h 22m 34s\tremaining: 2h 11m 50s\n",
      "3900:\tlearn: 0.8087479\ttest: 0.8300128\tbest: 0.8307205 (3800)\ttotal: 1h 23m 42s\tremaining: 2h 10m 52s\n",
      "3950:\tlearn: 0.8098030\ttest: 0.8296580\tbest: 0.8307205 (3800)\ttotal: 1h 24m 52s\tremaining: 2h 9m 56s\n",
      "4000:\tlearn: 0.8106883\ttest: 0.8300873\tbest: 0.8307205 (3800)\ttotal: 1h 26m 1s\tremaining: 2h 8m 59s\n",
      "4050:\tlearn: 0.8116062\ttest: 0.8302582\tbest: 0.8307205 (3800)\ttotal: 1h 27m 10s\tremaining: 2h 8m 1s\n",
      "4100:\tlearn: 0.8123007\ttest: 0.8306693\tbest: 0.8307205 (3800)\ttotal: 1h 28m 22s\tremaining: 2h 7m 6s\n",
      "4150:\tlearn: 0.8129791\ttest: 0.8298453\tbest: 0.8307205 (3800)\ttotal: 1h 29m 34s\tremaining: 2h 6m 12s\n",
      "4200:\tlearn: 0.8136728\ttest: 0.8317491\tbest: 0.8317491 (4200)\ttotal: 1h 30m 45s\tremaining: 2h 5m 17s\n",
      "4250:\tlearn: 0.8149143\ttest: 0.8318170\tbest: 0.8318170 (4250)\ttotal: 1h 31m 57s\tremaining: 2h 4m 21s\n",
      "4300:\tlearn: 0.8152399\ttest: 0.8321396\tbest: 0.8321396 (4300)\ttotal: 1h 33m 6s\tremaining: 2h 3m 22s\n",
      "4350:\tlearn: 0.8160220\ttest: 0.8325985\tbest: 0.8325985 (4350)\ttotal: 1h 34m 15s\tremaining: 2h 2m 22s\n",
      "4400:\tlearn: 0.8170160\ttest: 0.8325393\tbest: 0.8325985 (4350)\ttotal: 1h 35m 25s\tremaining: 2h 1m 24s\n",
      "4450:\tlearn: 0.8179491\ttest: 0.8323767\tbest: 0.8325985 (4350)\ttotal: 1h 36m 32s\tremaining: 2h 21s\n",
      "4500:\tlearn: 0.8184480\ttest: 0.8330673\tbest: 0.8330673 (4500)\ttotal: 1h 37m 42s\tremaining: 1h 59m 22s\n",
      "4550:\tlearn: 0.8191928\ttest: 0.8339028\tbest: 0.8339028 (4550)\ttotal: 1h 38m 52s\tremaining: 1h 58m 22s\n",
      "4600:\tlearn: 0.8199425\ttest: 0.8340663\tbest: 0.8340663 (4600)\ttotal: 1h 40m 2s\tremaining: 1h 57m 23s\n",
      "4650:\tlearn: 0.8206636\ttest: 0.8342694\tbest: 0.8342694 (4650)\ttotal: 1h 41m 13s\tremaining: 1h 56m 24s\n",
      "4700:\tlearn: 0.8213905\ttest: 0.8343797\tbest: 0.8343797 (4700)\ttotal: 1h 42m 21s\tremaining: 1h 55m 22s\n",
      "4750:\tlearn: 0.8222594\ttest: 0.8344647\tbest: 0.8344647 (4750)\ttotal: 1h 43m 32s\tremaining: 1h 54m 23s\n",
      "4800:\tlearn: 0.8229867\ttest: 0.8341089\tbest: 0.8344647 (4750)\ttotal: 1h 44m 42s\tremaining: 1h 53m 23s\n",
      "4850:\tlearn: 0.8233838\ttest: 0.8346868\tbest: 0.8346868 (4850)\ttotal: 1h 45m 51s\tremaining: 1h 52m 22s\n",
      "4900:\tlearn: 0.8240932\ttest: 0.8355763\tbest: 0.8355763 (4900)\ttotal: 1h 47m 4s\tremaining: 1h 51m 23s\n",
      "4950:\tlearn: 0.8246141\ttest: 0.8353664\tbest: 0.8355763 (4900)\ttotal: 1h 48m 15s\tremaining: 1h 50m 24s\n",
      "5000:\tlearn: 0.8253423\ttest: 0.8347379\tbest: 0.8355763 (4900)\ttotal: 1h 49m 21s\tremaining: 1h 49m 18s\n",
      "5050:\tlearn: 0.8260704\ttest: 0.8350870\tbest: 0.8355763 (4900)\ttotal: 1h 50m 25s\tremaining: 1h 48m 11s\n",
      "5100:\tlearn: 0.8269025\ttest: 0.8352474\tbest: 0.8355763 (4900)\ttotal: 1h 51m 29s\tremaining: 1h 47m 4s\n",
      "5150:\tlearn: 0.8275567\ttest: 0.8355898\tbest: 0.8355898 (5150)\ttotal: 1h 52m 32s\tremaining: 1h 45m 56s\n",
      "5200:\tlearn: 0.8284257\ttest: 0.8362491\tbest: 0.8362491 (5200)\ttotal: 1h 53m 37s\tremaining: 1h 44m 50s\n",
      "5250:\tlearn: 0.8291541\ttest: 0.8362223\tbest: 0.8362491 (5200)\ttotal: 1h 54m 39s\tremaining: 1h 43m 41s\n",
      "5300:\tlearn: 0.8297522\ttest: 0.8360505\tbest: 0.8362491 (5200)\ttotal: 1h 55m 42s\tremaining: 1h 42m 34s\n",
      "5350:\tlearn: 0.8305126\ttest: 0.8361826\tbest: 0.8362491 (5200)\ttotal: 1h 56m 45s\tremaining: 1h 41m 26s\n",
      "5400:\tlearn: 0.8311487\ttest: 0.8363957\tbest: 0.8363957 (5400)\ttotal: 1h 57m 49s\tremaining: 1h 40m 19s\n",
      "5450:\tlearn: 0.8321075\ttest: 0.8361991\tbest: 0.8363957 (5400)\ttotal: 1h 58m 53s\tremaining: 1h 39m 12s\n",
      "5500:\tlearn: 0.8329828\ttest: 0.8357077\tbest: 0.8363957 (5400)\ttotal: 1h 59m 56s\tremaining: 1h 38m 5s\n",
      "5550:\tlearn: 0.8336386\ttest: 0.8365473\tbest: 0.8365473 (5550)\ttotal: 2h 59s\tremaining: 1h 36m 58s\n",
      "5600:\tlearn: 0.8344408\ttest: 0.8365414\tbest: 0.8365473 (5550)\ttotal: 2h 2m 10s\tremaining: 1h 35m 57s\n",
      "5650:\tlearn: 0.8352116\ttest: 0.8374075\tbest: 0.8374075 (5650)\ttotal: 2h 3m 16s\tremaining: 1h 34m 52s\n",
      "5700:\tlearn: 0.8359231\ttest: 0.8374503\tbest: 0.8374503 (5700)\ttotal: 2h 4m 17s\tremaining: 1h 33m 43s\n",
      "5750:\tlearn: 0.8366599\ttest: 0.8367825\tbest: 0.8374503 (5700)\ttotal: 2h 5m 19s\tremaining: 1h 32m 35s\n",
      "5800:\tlearn: 0.8373801\ttest: 0.8371772\tbest: 0.8374503 (5700)\ttotal: 2h 6m 21s\tremaining: 1h 31m 28s\n",
      "5850:\tlearn: 0.8382294\ttest: 0.8371772\tbest: 0.8374503 (5700)\ttotal: 2h 7m 24s\tremaining: 1h 30m 20s\n",
      "5900:\tlearn: 0.8393576\ttest: 0.8370072\tbest: 0.8374503 (5700)\ttotal: 2h 8m 27s\tremaining: 1h 29m 14s\n",
      "5950:\tlearn: 0.8397336\ttest: 0.8363690\tbest: 0.8374503 (5700)\ttotal: 2h 9m 32s\tremaining: 1h 28m 8s\n",
      "6000:\tlearn: 0.8405272\ttest: 0.8365555\tbest: 0.8374503 (5700)\ttotal: 2h 10m 38s\tremaining: 1h 27m 3s\n",
      "6050:\tlearn: 0.8415916\ttest: 0.8365534\tbest: 0.8374503 (5700)\ttotal: 2h 11m 47s\tremaining: 1h 26m\n",
      "6100:\tlearn: 0.8424918\ttest: 0.8367845\tbest: 0.8374503 (5700)\ttotal: 2h 12m 53s\tremaining: 1h 24m 55s\n",
      "6150:\tlearn: 0.8430799\ttest: 0.8369731\tbest: 0.8374503 (5700)\ttotal: 2h 13m 56s\tremaining: 1h 23m 49s\n",
      "6200:\tlearn: 0.8437461\ttest: 0.8371341\tbest: 0.8374503 (5700)\ttotal: 2h 15m\tremaining: 1h 22m 42s\n",
      "6250:\tlearn: 0.8446749\ttest: 0.8373061\tbest: 0.8374503 (5700)\ttotal: 2h 16m 3s\tremaining: 1h 21m 36s\n",
      "6300:\tlearn: 0.8453679\ttest: 0.8375615\tbest: 0.8375615 (6300)\ttotal: 2h 17m 6s\tremaining: 1h 20m 29s\n",
      "6350:\tlearn: 0.8461245\ttest: 0.8373639\tbest: 0.8375615 (6300)\ttotal: 2h 18m 10s\tremaining: 1h 19m 23s\n",
      "6400:\tlearn: 0.8465793\ttest: 0.8371923\tbest: 0.8375615 (6300)\ttotal: 2h 19m 18s\tremaining: 1h 18m 19s\n",
      "6450:\tlearn: 0.8474017\ttest: 0.8377686\tbest: 0.8377686 (6450)\ttotal: 2h 20m 21s\tremaining: 1h 17m 13s\n",
      "6500:\tlearn: 0.8482836\ttest: 0.8365957\tbest: 0.8377686 (6450)\ttotal: 2h 21m 27s\tremaining: 1h 16m 7s\n",
      "6550:\tlearn: 0.8488748\ttest: 0.8373964\tbest: 0.8377686 (6450)\ttotal: 2h 22m 31s\tremaining: 1h 15m 2s\n",
      "6600:\tlearn: 0.8492614\ttest: 0.8373965\tbest: 0.8377686 (6450)\ttotal: 2h 23m 33s\tremaining: 1h 13m 55s\n",
      "6650:\tlearn: 0.8502052\ttest: 0.8377663\tbest: 0.8377686 (6450)\ttotal: 2h 24m 37s\tremaining: 1h 12m 49s\n",
      "6700:\tlearn: 0.8510777\ttest: 0.8375383\tbest: 0.8377686 (6450)\ttotal: 2h 25m 40s\tremaining: 1h 11m 42s\n",
      "6750:\tlearn: 0.8519850\ttest: 0.8376578\tbest: 0.8377686 (6450)\ttotal: 2h 26m 44s\tremaining: 1h 10m 37s\n",
      "6800:\tlearn: 0.8525839\ttest: 0.8376599\tbest: 0.8377686 (6450)\ttotal: 2h 27m 48s\tremaining: 1h 9m 31s\n",
      "6850:\tlearn: 0.8531928\ttest: 0.8376941\tbest: 0.8377686 (6450)\ttotal: 2h 28m 52s\tremaining: 1h 8m 25s\n",
      "6900:\tlearn: 0.8539825\ttest: 0.8382571\tbest: 0.8382571 (6900)\ttotal: 2h 29m 56s\tremaining: 1h 7m 20s\n",
      "6950:\tlearn: 0.8549270\ttest: 0.8380239\tbest: 0.8382571 (6900)\ttotal: 2h 31m 1s\tremaining: 1h 6m 14s\n",
      "7000:\tlearn: 0.8553682\ttest: 0.8375588\tbest: 0.8382571 (6900)\ttotal: 2h 32m 4s\tremaining: 1h 5m 8s\n",
      "7050:\tlearn: 0.8563254\ttest: 0.8374118\tbest: 0.8382571 (6900)\ttotal: 2h 33m 8s\tremaining: 1h 4m 3s\n",
      "7100:\tlearn: 0.8570195\ttest: 0.8378177\tbest: 0.8382571 (6900)\ttotal: 2h 34m 12s\tremaining: 1h 2m 57s\n",
      "7150:\tlearn: 0.8578647\ttest: 0.8380438\tbest: 0.8382571 (6900)\ttotal: 2h 35m 17s\tremaining: 1h 1m 52s\n",
      "7200:\tlearn: 0.8588015\ttest: 0.8378753\tbest: 0.8382571 (6900)\ttotal: 2h 36m 21s\tremaining: 1h 46s\n",
      "7250:\tlearn: 0.8592696\ttest: 0.8380946\tbest: 0.8382571 (6900)\ttotal: 2h 37m 28s\tremaining: 59m 42s\n",
      "7300:\tlearn: 0.8597837\ttest: 0.8385298\tbest: 0.8385298 (7300)\ttotal: 2h 38m 36s\tremaining: 58m 38s\n",
      "7350:\tlearn: 0.8607716\ttest: 0.8383584\tbest: 0.8385298 (7300)\ttotal: 2h 39m 42s\tremaining: 57m 33s\n",
      "7400:\tlearn: 0.8613993\ttest: 0.8384354\tbest: 0.8385298 (7300)\ttotal: 2h 40m 46s\tremaining: 56m 27s\n",
      "7450:\tlearn: 0.8622039\ttest: 0.8380176\tbest: 0.8385298 (7300)\ttotal: 2h 41m 52s\tremaining: 55m 22s\n",
      "7500:\tlearn: 0.8629240\ttest: 0.8379760\tbest: 0.8385298 (7300)\ttotal: 2h 42m 57s\tremaining: 54m 17s\n",
      "7550:\tlearn: 0.8639520\ttest: 0.8382036\tbest: 0.8385298 (7300)\ttotal: 2h 44m 1s\tremaining: 53m 11s\n",
      "7600:\tlearn: 0.8646810\ttest: 0.8384061\tbest: 0.8385298 (7300)\ttotal: 2h 45m 4s\tremaining: 52m 5s\n",
      "7650:\tlearn: 0.8652752\ttest: 0.8387407\tbest: 0.8387407 (7650)\ttotal: 2h 46m 7s\tremaining: 51m\n",
      "7700:\tlearn: 0.8658387\ttest: 0.8392597\tbest: 0.8392597 (7700)\ttotal: 2h 47m 11s\tremaining: 49m 54s\n",
      "7750:\tlearn: 0.8668775\ttest: 0.8386763\tbest: 0.8392597 (7700)\ttotal: 2h 48m 22s\tremaining: 48m 51s\n",
      "7800:\tlearn: 0.8677359\ttest: 0.8390979\tbest: 0.8392597 (7700)\ttotal: 2h 49m 30s\tremaining: 47m 46s\n",
      "7850:\tlearn: 0.8683802\ttest: 0.8393398\tbest: 0.8393398 (7850)\ttotal: 2h 50m 34s\tremaining: 46m 41s\n",
      "7900:\tlearn: 0.8693602\ttest: 0.8391720\tbest: 0.8393398 (7850)\ttotal: 2h 51m 37s\tremaining: 45m 35s\n",
      "7950:\tlearn: 0.8699041\ttest: 0.8395065\tbest: 0.8395065 (7950)\ttotal: 2h 52m 42s\tremaining: 44m 30s\n",
      "8000:\tlearn: 0.8703234\ttest: 0.8393113\tbest: 0.8395065 (7950)\ttotal: 2h 53m 48s\tremaining: 43m 25s\n",
      "8050:\tlearn: 0.8713609\ttest: 0.8391394\tbest: 0.8395065 (7950)\ttotal: 2h 54m 52s\tremaining: 42m 19s\n",
      "8100:\tlearn: 0.8721705\ttest: 0.8393343\tbest: 0.8395065 (7950)\ttotal: 2h 55m 58s\tremaining: 41m 14s\n",
      "8150:\tlearn: 0.8729764\ttest: 0.8393292\tbest: 0.8395065 (7950)\ttotal: 2h 57m 2s\tremaining: 40m 9s\n",
      "8200:\tlearn: 0.8733250\ttest: 0.8395189\tbest: 0.8395189 (8200)\ttotal: 2h 58m 7s\tremaining: 39m 4s\n",
      "8250:\tlearn: 0.8742431\ttest: 0.8397550\tbest: 0.8397550 (8250)\ttotal: 2h 59m 11s\tremaining: 37m 59s\n",
      "8300:\tlearn: 0.8747926\ttest: 0.8397320\tbest: 0.8397550 (8250)\ttotal: 3h 15s\tremaining: 36m 53s\n",
      "8350:\tlearn: 0.8759297\ttest: 0.8397483\tbest: 0.8397550 (8250)\ttotal: 3h 1m 21s\tremaining: 35m 48s\n",
      "8400:\tlearn: 0.8762702\ttest: 0.8389548\tbest: 0.8397550 (8250)\ttotal: 3h 2m 25s\tremaining: 34m 43s\n",
      "8450:\tlearn: 0.8770517\ttest: 0.8389454\tbest: 0.8397550 (8250)\ttotal: 3h 3m 29s\tremaining: 33m 38s\n",
      "8500:\tlearn: 0.8778558\ttest: 0.8387313\tbest: 0.8397550 (8250)\ttotal: 3h 4m 33s\tremaining: 32m 32s\n",
      "8550:\tlearn: 0.8785915\ttest: 0.8391189\tbest: 0.8397550 (8250)\ttotal: 3h 5m 36s\tremaining: 31m 27s\n",
      "8600:\tlearn: 0.8793178\ttest: 0.8387092\tbest: 0.8397550 (8250)\ttotal: 3h 6m 50s\tremaining: 30m 23s\n",
      "8650:\tlearn: 0.8797964\ttest: 0.8393296\tbest: 0.8397550 (8250)\ttotal: 3h 7m 56s\tremaining: 29m 18s\n",
      "8700:\tlearn: 0.8805810\ttest: 0.8395625\tbest: 0.8397550 (8250)\ttotal: 3h 9m 2s\tremaining: 28m 13s\n",
      "8750:\tlearn: 0.8811939\ttest: 0.8395788\tbest: 0.8397550 (8250)\ttotal: 3h 10m 7s\tremaining: 27m 8s\n",
      "8800:\tlearn: 0.8817449\ttest: 0.8403400\tbest: 0.8403400 (8800)\ttotal: 3h 11m 12s\tremaining: 26m 2s\n",
      "8850:\tlearn: 0.8826355\ttest: 0.8403096\tbest: 0.8403400 (8800)\ttotal: 3h 12m 17s\tremaining: 24m 57s\n",
      "8900:\tlearn: 0.8832022\ttest: 0.8401029\tbest: 0.8403400 (8800)\ttotal: 3h 13m 19s\tremaining: 23m 52s\n",
      "8950:\tlearn: 0.8836739\ttest: 0.8396896\tbest: 0.8403400 (8800)\ttotal: 3h 14m 22s\tremaining: 22m 46s\n",
      "9000:\tlearn: 0.8847011\ttest: 0.8400580\tbest: 0.8403400 (8800)\ttotal: 3h 15m 28s\tremaining: 21m 41s\n",
      "9050:\tlearn: 0.8852927\ttest: 0.8394539\tbest: 0.8403400 (8800)\ttotal: 3h 16m 32s\tremaining: 20m 36s\n",
      "9100:\tlearn: 0.8856677\ttest: 0.8395323\tbest: 0.8403400 (8800)\ttotal: 3h 17m 36s\tremaining: 19m 31s\n",
      "9150:\tlearn: 0.8864369\ttest: 0.8394624\tbest: 0.8403400 (8800)\ttotal: 3h 18m 40s\tremaining: 18m 25s\n",
      "9200:\tlearn: 0.8874323\ttest: 0.8398577\tbest: 0.8403400 (8800)\ttotal: 3h 19m 45s\tremaining: 17m 20s\n",
      "9250:\tlearn: 0.8881902\ttest: 0.8398102\tbest: 0.8403400 (8800)\ttotal: 3h 20m 51s\tremaining: 16m 15s\n",
      "9300:\tlearn: 0.8887680\ttest: 0.8394635\tbest: 0.8403400 (8800)\ttotal: 3h 22m 3s\tremaining: 15m 11s\n",
      "9350:\tlearn: 0.8891919\ttest: 0.8398128\tbest: 0.8403400 (8800)\ttotal: 3h 23m 16s\tremaining: 14m 6s\n",
      "9400:\tlearn: 0.8901966\ttest: 0.8397200\tbest: 0.8403400 (8800)\ttotal: 3h 24m 29s\tremaining: 13m 1s\n",
      "9450:\tlearn: 0.8910303\ttest: 0.8398869\tbest: 0.8403400 (8800)\ttotal: 3h 25m 40s\tremaining: 11m 56s\n",
      "9500:\tlearn: 0.8916627\ttest: 0.8403773\tbest: 0.8403773 (9500)\ttotal: 3h 26m 52s\tremaining: 10m 51s\n",
      "9550:\tlearn: 0.8921569\ttest: 0.8401344\tbest: 0.8403773 (9500)\ttotal: 3h 28m 3s\tremaining: 9m 46s\n",
      "9600:\tlearn: 0.8926053\ttest: 0.8401327\tbest: 0.8403773 (9500)\ttotal: 3h 29m 16s\tremaining: 8m 41s\n",
      "9650:\tlearn: 0.8930458\ttest: 0.8394744\tbest: 0.8403773 (9500)\ttotal: 3h 30m 28s\tremaining: 7m 36s\n",
      "9700:\tlearn: 0.8937066\ttest: 0.8394751\tbest: 0.8403773 (9500)\ttotal: 3h 31m 42s\tremaining: 6m 31s\n",
      "9750:\tlearn: 0.8940002\ttest: 0.8392502\tbest: 0.8403773 (9500)\ttotal: 3h 32m 58s\tremaining: 5m 26s\n",
      "9800:\tlearn: 0.8948368\ttest: 0.8393621\tbest: 0.8403773 (9500)\ttotal: 3h 34m 11s\tremaining: 4m 20s\n",
      "9850:\tlearn: 0.8954179\ttest: 0.8394096\tbest: 0.8403773 (9500)\ttotal: 3h 35m 22s\tremaining: 3m 15s\n",
      "9900:\tlearn: 0.8960313\ttest: 0.8396366\tbest: 0.8403773 (9500)\ttotal: 3h 36m 32s\tremaining: 2m 9s\n",
      "9950:\tlearn: 0.8968597\ttest: 0.8396430\tbest: 0.8403773 (9500)\ttotal: 3h 37m 38s\tremaining: 1m 4s\n",
      "9999:\tlearn: 0.8973289\ttest: 0.8401856\tbest: 0.8403773 (9500)\ttotal: 3h 38m 42s\tremaining: 0us\n",
      "bestTest = 0.8403773431\n",
      "bestIteration = 9500\n",
      "Shrink model to first 9501 iterations.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x22b69a02580>"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "train_data = Pool(data = train_dataset[features],\n",
    "                label = train_dataset[target],\n",
    "                cat_features = cat_features,\n",
    "                text_features = text_features\n",
    "                )\n",
    "\n",
    "valid_data = Pool(data = valid_dataset[features],\n",
    "                label = valid_dataset[target],\n",
    "                cat_features = cat_features,\n",
    "                text_features = text_features\n",
    "                )\n",
    "\n",
    "model = CatBoostClassifier(**params)\n",
    "model.fit(train_data, eval_set = valid_data, plot = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Feature Id  Importances\n",
       "0          item_name    54.990915\n",
       "1         first_word    19.276928\n",
       "2          last_word     7.241995\n",
       "3  second_first_word     5.397398\n",
       "4   second_last_word     4.607832\n",
       "5      item_nds_rate     3.745917\n",
       "6   uppercase_amount     1.468260\n",
       "7         item_price     1.272674\n",
       "8      string_length     0.580896\n",
       "9  receipt_dayofweek     0.281050"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature Id</th>\n      <th>Importances</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>item_name</td>\n      <td>54.990915</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>first_word</td>\n      <td>19.276928</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>last_word</td>\n      <td>7.241995</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>second_first_word</td>\n      <td>5.397398</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>second_last_word</td>\n      <td>4.607832</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>item_nds_rate</td>\n      <td>3.745917</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>uppercase_amount</td>\n      <td>1.468260</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>item_price</td>\n      <td>1.272674</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>string_length</td>\n      <td>0.580896</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>receipt_dayofweek</td>\n      <td>0.281050</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "feature_importance = model.get_feature_importance(prettified = True).head(10)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['id'] = '0'\n",
    "dataset.to_parquet('data/task1_test_for_user.parquet',\n",
    "                compression='gzip'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving label encoder and model\n",
    "import pickle\n",
    "\n",
    "with open(\"encoder.pkl\", \"wb\") as out_labelecoder:\n",
    "    pickle.dump(labelecoder, out_labelecoder)\n",
    "\n",
    "with open(\"model.clf\", 'wb') as out_classifier:\n",
    "    out_classifier.write(pickle.dumps(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#preproccesses the dataframe, generates features\n",
    "def preproccess(dataset):\n",
    "     replace_dict = {\n",
    "          \"w\" : \"ш\",\n",
    "          \"e\" : \"е\",\n",
    "          \"y\" : \"у\",\n",
    "          \"u\" : \"и\",\n",
    "          \"o\" : \"о\",\n",
    "          \"p\" : \"р\",\n",
    "          \"a\" : \"а\",\n",
    "          \"h\" : \"н\",\n",
    "          \"k\" : \"к\",\n",
    "          \"x\" : \"х\",\n",
    "          \"c\" : \"с\",\n",
    "          \"b\" : \"в\",\n",
    "          \"m\" : \"м\",\n",
    "          \"r\" : \"г\",\n",
    "          \"a\" : \"a\",\n",
    "          \"a\" : \"a\",          \n",
    "     }\n",
    "     #replaces latin letters in cyrillic words\n",
    "     def replace_latin(text):\n",
    "          if len(text) > 0:\n",
    "               text = text.split()\n",
    "               words = []\n",
    "               for word in text:\n",
    "                    if re.match(r\"^[a-z]+$\", word):\n",
    "                         pass\n",
    "                    else:\n",
    "                         for lat, ru in replace_dict.items():\n",
    "                              word = word.replace(lat, ru)\n",
    "                    words.append(word)\n",
    "               \n",
    "               return \" \".join(words)\n",
    "          else:\n",
    "               return \"none\"\n",
    "     #separates numbers and punctuation from words\n",
    "     def pretokenize_test(text):          \n",
    "          text = text.replace(\"-\", \"\")\n",
    "          text = re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", text).strip()\n",
    "          text = \"\".join(c if c not in string.punctuation else f\" {c} \" for c in text )\n",
    "          text = \"\".join(c if c not in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] else f\" {c} \" for c in text)\n",
    "          #text = re.sub(r'[0-9]', ' 1 ', text)\n",
    "          text =  \" \".join(w.strip() for w in text.split())         \n",
    "\n",
    "          return text\n",
    "     #removes all numbers and punctuation\n",
    "     def clean_text(text):\n",
    "          text = re.sub(r'[?|!\"|#|.|%|,|$|~|^|`|;|@|╣|)|(|\\\\|\\||-|[|\\]|{|}|:|<|>|\\'|+|&|=|°|/|№|\\-|*|_]', r\" \", text)\n",
    "          text = re.sub(r'[0-9]', '', text)          \n",
    "          return text\n",
    "     #removes all words shorter than 2 letters\n",
    "     def keep_long_words(text):\n",
    "          words = []\n",
    "          for word in text.split():\n",
    "               if len(word) > 2:\n",
    "                    words.append(word)\n",
    "          if len(words) == 0:\n",
    "               return \"none\"\n",
    "          return \" \".join(words)\n",
    "\n",
    "     #split time into hours and minutes\n",
    "     time_list = [x.split(\":\") for x in dataset['receipt_time'].values]\n",
    "     dataset['receipt_time_hours'] = [row[0] for row in time_list]\n",
    "     dataset['receipt_time_minutes'] = [row[1] for row in time_list]\n",
    "     #drop unneccessary \"receipt time\" column\n",
    "     dataset.drop([\"receipt_time\"], inplace = True, axis = 1)\n",
    "\n",
    "     #creating features\n",
    "     #order of execution is important\n",
    "     dataset[\"string_length\"] = dataset[\"item_name\"].apply(lambda x: len(x))\n",
    "     dataset[\"punctuation_count\"] = dataset[\"item_name\"].apply(lambda comment: sum(comment.count(w) for w in '.,;:-/\\\\'))\n",
    "     dataset[\"uppercase_amount\"] = dataset[\"item_name\"].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.lower())\n",
    "     dataset[\"digit_amount\"] = dataset[\"item_name\"].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: pretokenize_test(x))\n",
    "\n",
    "     dataset[\"latin_amount\"] = dataset[\"item_name\"].apply(lambda x: len(re.findall(r\"[a-z]\", x)))\n",
    "     dataset[\"cyrillic_amount\"] = dataset[\"item_name\"].apply(lambda x: len(re.findall(r\"[а-я]\", x)))          \n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"ё\", \"е\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"й\", \"и\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"ъ\", \"ь\"))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"tpk\", \"трк\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"трк\", \"топливо\"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\" аи \", \" бензин \"))\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: x.replace(\"раф\", \"кофе\"))\n",
    "\n",
    "     dataset[\"item_name\"] = dataset[\"item_name\"].apply(lambda x: replace_latin(x))\n",
    "     \n",
    "     dataset[\"temp_col\"] = dataset[\"item_name\"].apply(lambda x: clean_text(x))     \n",
    "     \n",
    "     dataset[\"has_kcal\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"ккал\" in x.split() else 0)\n",
    "     dataset[\"has_kg\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"кг\" in x.split() else 0)\n",
    "     dataset[\"has_g\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"г\" in x.split() else 0)\n",
    "     dataset[\"has_ml\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"мл\" in x.split() else 0)    \n",
    "     dataset[\"has_l\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"л\" in x.split() else 0)\n",
    "     dataset[\"has_sht\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"шт\" in x.split() else 0)\n",
    "     dataset[\"has_tab\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"таб\" in x.split() else 0)\n",
    "     dataset[\"has_sb\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"сб\" in x.split() else 0)\n",
    "\n",
    "     dataset[\"has_fl\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"фл\" in x.split() else 0)\n",
    "     dataset[\"has_up\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"уп\" in x.split() else 0)\n",
    "     dataset[\"has_cm\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"см\" in x.split() else 0)\n",
    "     dataset[\"has_m\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"м\" in x.split() else 0)\n",
    "     dataset[\"has_mm\"] = dataset[\"temp_col\"].apply(lambda x: 1 if \"мм\" in x.split() else 0)\n",
    "\n",
    "     dataset[\"temp_col\"] = dataset[\"temp_col\"].apply(lambda x: keep_long_words(x))\n",
    "\n",
    "     dataset[\"first_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[0] if len(x.split()) > 0 else \"none\")\n",
    "     dataset[\"last_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[-1] if len(x.split()) > 0 else \"none\")\n",
    "\n",
    "     dataset[\"second_first_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[1] if len(x.split()) > 1 else \"none\")\n",
    "     dataset[\"second_last_word\"] = dataset[\"temp_col\"].apply(lambda x: x.split()[-2] if len(x.split()) > 1 else \"none\")\n",
    "\n",
    "     dataset.drop(\"temp_col\", axis = 1, inplace = True)\n",
    "\n",
    "     return dataset\n",
    "\n",
    "#lower dtypes to save memory\n",
    "def lower_dtypes(dataset):\n",
    "\n",
    "    dtype_dict = {\n",
    "    \"has_kcal\" : np.int8,\n",
    "    \"has_kg\" : np.int8,\n",
    "    \"has_g\" : np.int8,\n",
    "    \"has_ml\" : np.int8,\n",
    "    \"has_l\" : np.int8,\n",
    "    \"has_sht\" : np.int8,\n",
    "    \"has_tab\" : np.int8,\n",
    "    \"has_sb\" : np.int8,\n",
    "    \"has_fl\" : np.int8,\n",
    "    \"has_up\" : np.int8,\n",
    "    \"has_cm\" : np.int8,\n",
    "    \"has_m\" : np.int8,\n",
    "    \"has_mm\" : np.int8,\n",
    "    \"receipt_id\" : np.int32,\n",
    "    \"id\" : np.int32,\n",
    "    \"receipt_dayofweek\" : np.int8,\n",
    "    \"one_char_amount\" : np.int8,\n",
    "    \"two_char_amount\" : np.int8,\n",
    "    \"item_name\" : str,\n",
    "    \"item_name_modified\" : str,\n",
    "    \"item_quantity\" : np.int8,\n",
    "    \"item_price\" : np.int8,\n",
    "    \"item_nds_rate\" : np.int8,\n",
    "    \"category_id\" : np.int8,\n",
    "    \"receipt_time_hours\" : np.int8,\n",
    "    \"receipt_time_minutes\" : np.int8,\n",
    "    \"string_length\" : np.float16,\n",
    "    \"punctuation_count\" : np.int8,\n",
    "    \"uppercase_amount\" : np.int8,\n",
    "    \"FULLCAPS_COUNT\" : np.float16,\n",
    "    \"digit_amount\" : np.int8,\n",
    "    \"relative_digit_amount\" : np.float16,\n",
    "    \"latin_amount\" : np.int8,\n",
    "    \"cyrillic_amount\" : np.int8,\n",
    "    \"relative_latin_amount\" : np.float16,\n",
    "    \"relative_cyrillic_amount\" : np.float16,\n",
    "    \"first_word\" : str,\n",
    "    \"last_word\" : str,\n",
    "    \"second_first_word\" : str,\n",
    "    \"second_last_word\" : str,\n",
    "    \"item_name_list\" : str,\n",
    "    \"two_letter_features\" : str,\n",
    "    \"one_letter_features\" : str,\n",
    "    \"brands\" : str\n",
    "    }\n",
    "   \n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].astype(dtype_dict[column])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#splitting features into categorical, continuous and text\n",
    "cont_features = [\n",
    "                \"string_length\",\n",
    "                \"punctuation_count\",\n",
    "                \"uppercase_amount\",\n",
    "                \"digit_amount\",\n",
    "                \"latin_amount\",\n",
    "                \"cyrillic_amount\",\n",
    "                \"item_quantity\",\n",
    "                \"item_price\"\n",
    "                ]\n",
    "\n",
    "cat_features = [\n",
    "                \"has_kcal\",\n",
    "                \"has_kg\",\n",
    "                \"has_g\",\n",
    "                \"has_ml\",\n",
    "                \"has_l\",\n",
    "                \"has_sht\",\n",
    "                \"has_tab\",\n",
    "                \"has_sb\",\n",
    "                \"has_fl\",\n",
    "                \"has_up\",\n",
    "                \"has_cm\",\n",
    "                \"has_m\",\n",
    "                \"has_mm\",\n",
    "                \"item_nds_rate\",                             \n",
    "                \"receipt_time_hours\",\n",
    "                \"receipt_time_minutes\",                \n",
    "                \"receipt_dayofweek\",\n",
    "                #\"receipt_id\"\n",
    "                ]\n",
    "\n",
    "text_features = [\"item_name\", \"first_word\", \"last_word\", \"second_first_word\", \"second_last_word\"]\n",
    "target = [\"category_id\"]\n",
    "features = text_features + cat_features + cont_features\n",
    "\n",
    "#custom text proccessing\n",
    "#text processing differs for item_name and other text columns\n",
    "text_processing = {\n",
    "        \"tokenizers\" : [\n",
    "          {\n",
    "            \"tokenizer_id\" : \"Space\",\n",
    "            \"separator_type\" : \"ByDelimiter\",\n",
    "            \"delimiter\" : \" \",\n",
    "            \"number_process_policy\" : \"Replace\",\n",
    "            \"number_token\" : \"@\",\n",
    "          },          \n",
    "        ],\n",
    "\n",
    "        \"dictionaries\" : [\n",
    "            {            \n",
    "              \"dictionary_id\" : \"Unigram\",\n",
    "              \"token_level_type\": \"Letter\",\n",
    "              #\"max_dictionary_size\" : \"500\",\n",
    "              \"occurrence_lower_bound\" : \"1\",\n",
    "              \"gram_order\" : \"1\"\n",
    "            },          \n",
    "            {\n",
    "            \"dictionary_id\" : \"Bigram\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"2\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Trigram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"3\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Fourgram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"4\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Fivegram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"5\"\n",
    "            },\n",
    "            {\n",
    "            \"dictionary_id\" : \"Sixgram\",\n",
    "            #\"max_dictionary_size\" : \"500\",\n",
    "            \"token_level_type\": \"Letter\",\n",
    "            \"occurrence_lower_bound\" : \"1\",\n",
    "            \"gram_order\" : \"6\"\n",
    "            },   \n",
    "        ],\n",
    "\n",
    "        \"feature_processing\" : {\n",
    "            \"item_name\" : [                  \n",
    "                  {\n",
    "                    \"dictionaries_names\" : [\n",
    "                      #\"Unigram\",                      \n",
    "                      #\"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BoW\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [ \n",
    "                      #\"Unigram\",                     \n",
    "                      #\"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"NaiveBayes\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [                      \n",
    "                      #\"Unigram\",\n",
    "                      #\"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BM25\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "            ],\n",
    "\n",
    "            \"default\" : [                  \n",
    "                  {\n",
    "                    \"dictionaries_names\" : [\n",
    "                      #\"Unigram\",                      \n",
    "                      \"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BoW\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [ \n",
    "                      #\"Unigram\",                     \n",
    "                      \"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"NaiveBayes\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "                  {\n",
    "                    \"dictionaries_names\" : [                      \n",
    "                      #\"Unigram\",\n",
    "                      \"Bigram\",\n",
    "                      \"Trigram\",\n",
    "                      \"Fourgram\",\n",
    "                      \"Fivegram\",\n",
    "                      #\"Sixgram\"\n",
    "                      ],\n",
    "                    \"feature_calcers\" : [\"BM25\"],\n",
    "                    \"tokenizers_names\" : [\"Space\"]\n",
    "                  },\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    dataset = pd.read_parquet('data/task1_test_for_user.parquet')\n",
    "\n",
    "    with open(\"model.clf\", 'rb') as f:\n",
    "        model = pickle.loads(f.read())\n",
    "\n",
    "    with open(\"encoder.pkl\", \"rb\") as f:\n",
    "        labelecoder = pickle.loads(f.read())\n",
    "    \n",
    "    dataset = preproccess(dataset)\n",
    "    dataset = lower_dtypes(dataset)    \n",
    "\n",
    "    predictions = model.predict(dataset[features])\n",
    "    predictions = labelecoder.inverse_transform(predictions)\n",
    "    result = pd.DataFrame(predictions, columns=['pred'])\n",
    "    result['id'] = dataset['id'].values\n",
    "    result[['id', 'pred']].to_csv('answers.csv', index=None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}